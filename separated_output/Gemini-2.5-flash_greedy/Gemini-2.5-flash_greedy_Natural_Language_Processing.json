[
  {
    "task_id": "ClassEval_52",
    "skeleton": "import nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag, word_tokenize\nimport string\n\n\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')\nnltk.download('wordnet')\n\nclass Lemmatization:\n    \"\"\"\n    This is a class about Lemmatization, which utilizes the nltk library to perform lemmatization and part-of-speech tagging on sentences, as well as remove punctuation.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        creates a WordNetLemmatizer object and stores it in the self.lemmatizer member variable.\n        \"\"\"\n        self.lemmatizer = WordNetLemmatizer()\n\n    def lemmatize_sentence(self, sentence):\n        \"\"\"\n        Remove punctuations of the sentence and tokenizes the input sentence, mark the part of speech tag of each word,\n        lemmatizes the words with different parameters based on their parts of speech, and stores in a list.\n        :param sentence: a sentence str\n        :return: a list of words which have been lemmatized.\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.lemmatize_sentence(\"I am running in a race.\")\n        ['I', 'be', 'run', 'in', 'a', 'race']\n\n        \"\"\"\n\n    def get_pos_tag(self, sentence):\n        \"\"\"\n        Remove punctuations of the sentence and tokenizes the input sentence, mark the part of speech tag of each word.\n        :param sentence: a sentence str\n        :return: list, part of speech tag of each word in the sentence.\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.get_pos_tag(\"I am running in a race.\")\n        ['PRP', 'VBP', 'VBG', 'IN', 'DT', 'NN']\n\n        \"\"\"\n\n    def remove_punctuation(self, sentence):\n        \"\"\"\n        Removes punctuation from the input text.\n        :param sentence: a sentence str\n        :return: str, sentence without any punctuation\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.remove_punctuation(\"I am running in a race.\")\n        'I am running in a race'\n\n        \"\"\"",
    "test": "import unittest\n\nclass LemmatizationTestLemmatizeSentence(unittest.TestCase):\n    def test_lemmatize_sentence_1(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"I am running in a race.\")\n        expected = ['I', 'be', 'run', 'in', 'a', 'race']\n        self.assertEqual(result, expected)\n\n    def test_lemmatize_sentence_2(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"Until the beating, Cantanco's eyesight had been weak, but adequate.\")\n        expected = ['Until', 'the', 'beating', 'Cantancos', 'eyesight', 'have', 'be', 'weak', 'but', 'adequate']\n        self.assertEqual(result, expected)\n\n    def test_lammatize_sentence_3(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"The dog's barked at the mailman.\")\n        expected = ['The', 'dog', 'bark', 'at', 'the', 'mailman']\n        self.assertEqual(result, expected)\n\n    def test_lemmatize_sentence_4(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"He was running and eating at same time. \")\n        expected = ['He', 'be', 'run', 'and', 'eat', 'at', 'same', 'time']\n        self.assertEqual(result, expected)\n\n    def test_lemmatize_sentence_5(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"I was taking a ride in the car.\")\n        expected = ['I', 'be', 'take', 'a', 'ride', 'in', 'the', 'car']\n        self.assertEqual(result, expected)\n\nclass LemmatizationTestGetPosTag(unittest.TestCase):\n    def test_get_pos_tag_1(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"I am running in a race.\")\n        expected = ['PRP', 'VBP', 'VBG', 'IN', 'DT', 'NN']\n        self.assertEqual(result, expected)\n\n    def test_get_pos_tag_2(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"Cantanco's eyesight had been weak, but adequate.\")\n        expected = ['NNP', 'NN', 'VBD', 'VBN', 'JJ', 'CC', 'JJ']\n        self.assertEqual(result, expected)\n\n    def test_get_pos_tag_3(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"The dog's barked at the mailman.\")\n        expected = ['DT', 'NNS', 'VBD', 'IN', 'DT', 'NN']\n        self.assertEqual(result, expected)\n\n    def test_get_pos_tag_4(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"He was running and eating at same time. \")\n        expected = ['PRP', 'VBD', 'VBG', 'CC', 'VBG', 'IN', 'JJ', 'NN']\n        self.assertEqual(result, expected)\n\n    def test_get_pos_tag_5(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"I was taking a ride in the car.\")\n        expected = ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN']\n        self.assertEqual(result, expected)\n\n\nclass LemmatizationTestRemovePunctuation(unittest.TestCase):\n    def test_remove_punctuation_1(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"I am running in a race.\")\n        expected = \"I am running in a race\"\n        self.assertEqual(result, expected)\n\n    def test_remove_punctuation_2(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"Until the beating, Cantanco's eyesight had been weak, but adequate.\")\n        expected = 'Until the beating Cantancos eyesight had been weak but adequate'\n        self.assertEqual(result, expected)\n\n    def test_remove_punctuation_3(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"The dog's barked at the mailman!!!\")\n        expected = 'The dogs barked at the mailman'\n        self.assertEqual(result, expected)\n\n    def test_remove_punctuation_4(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"He was running and eating at same time... \")\n        expected = 'He was running and eating at same time '\n        self.assertEqual(result, expected)\n\n    def test_remove_punctuation_5(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"Is this a test? I hope it is...\")\n        expected = 'Is this a test I hope it is'\n        self.assertEqual(result, expected)\n\nclass LemmatizationTestMain(unittest.TestCase):\n    def test_main(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"Until the beating, Cantanco's eyesight had been weak, but adequate.\")\n        expected = ['Until', 'the', 'beating', 'Cantancos', 'eyesight', 'have', 'be', 'weak', 'but', 'adequate']\n        self.assertEqual(result, expected)\n\n        result = lemmatization.get_pos_tag(\"Cantanco's eyesight had been weak, but adequate.\")\n        expected = ['NNP', 'NN', 'VBD', 'VBN', 'JJ', 'CC', 'JJ']\n        self.assertEqual(result, expected)",
    "solution_code": "import nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag, word_tokenize\nimport string\n\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')\nnltk.download('wordnet')\n\n\nclass Lemmatization:\n    def __init__(self):\n        self.lemmatizer = WordNetLemmatizer()\n\n    def lemmatize_sentence(self, sentence):\n        lemmatized_words = []\n        sentence = self.remove_punctuation(sentence)\n        words = word_tokenize(sentence)\n        tagged_words = pos_tag(words)\n        for word, tag in tagged_words:\n            if tag.startswith('V'):\n                lemmatized_word = self.lemmatizer.lemmatize(word, pos='v')\n            elif tag.startswith('J'):\n                lemmatized_word = self.lemmatizer.lemmatize(word, pos='a')\n            elif tag.startswith('R'):\n                lemmatized_word = self.lemmatizer.lemmatize(word, pos='r')\n            else:\n                lemmatized_word = self.lemmatizer.lemmatize(word)\n            lemmatized_words.append(lemmatized_word)\n        return lemmatized_words\n\n    def get_pos_tag(self, sentence):\n        pos_tags = []\n        sentence = self.remove_punctuation(sentence)\n        words = word_tokenize(sentence)\n        tagged_words = pos_tag(words)\n        for tagged_word in tagged_words:\n            pos_tags.append(tagged_word[1])\n        return pos_tags\n\n    def remove_punctuation(self, sentence):\n        return sentence.translate(str.maketrans('', '', string.punctuation))",
    "import_statement": [
      "import nltk",
      "from nltk.stem import WordNetLemmatizer",
      "from nltk import pos_tag, word_tokenize",
      "import string"
    ],
    "class_description": "    \"\"\"\n    This is a class about Lemmatization, which utilizes the nltk library to perform lemmatization and part-of-speech tagging on sentences, as well as remove punctuation.\n    \"\"\"\n",
    "class_name": "Lemmatization",
    "test_classes": [
      "LemmatizationTestLemmatizeSentence",
      "LemmatizationTestGetPosTag",
      "LemmatizationTestRemovePunctuation",
      "LemmatizationTestMain"
    ],
    "class_constructor": "class Lemmatization: \n    def __init__(self):\n        \"\"\"\n        creates a WordNetLemmatizer object and stores it in the self.lemmatizer member variable.\n        \"\"\"\n        self.lemmatizer = WordNetLemmatizer()\n\n",
    "fields": [
      "self.lemmatizer"
    ],
    "methods_info": [
      {
        "method_name": "lemmatize_sentence",
        "method_description": "def lemmatize_sentence(self, sentence):\n        \"\"\"\n        Remove punctuations of the sentence and tokenizes the input sentence, mark the part of speech tag of each word,\n        lemmatizes the words with different parameters based on their parts of speech, and stores in a list.\n        :param sentence: a sentence str\n        :return: a list of words which have been lemmatized.\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.lemmatize_sentence(\"I am running in a race.\")\n        ['I', 'be', 'run', 'in', 'a', 'race']\n\n        \"\"\"",
        "test_class": "LemmatizationTestLemmatizeSentence",
        "test_code": "class LemmatizationTestLemmatizeSentence(unittest.TestCase):\n    def test_lemmatize_sentence_1(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"I am running in a race.\")\n        expected = ['I', 'be', 'run', 'in', 'a', 'race']\n        self.assertEqual(result, expected)\n\n    def test_lemmatize_sentence_2(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"Until the beating, Cantanco's eyesight had been weak, but adequate.\")\n        expected = ['Until', 'the', 'beating', 'Cantancos', 'eyesight', 'have', 'be', 'weak', 'but', 'adequate']\n        self.assertEqual(result, expected)\n\n    def test_lammatize_sentence_3(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"The dog's barked at the mailman.\")\n        expected = ['The', 'dog', 'bark', 'at', 'the', 'mailman']\n        self.assertEqual(result, expected)\n\n    def test_lemmatize_sentence_4(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"He was running and eating at same time. \")\n        expected = ['He', 'be', 'run', 'and', 'eat', 'at', 'same', 'time']\n        self.assertEqual(result, expected)\n\n    def test_lemmatize_sentence_5(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.lemmatize_sentence(\"I was taking a ride in the car.\")\n        expected = ['I', 'be', 'take', 'a', 'ride', 'in', 'the', 'car']\n        self.assertEqual(result, expected)",
        "solution_code": "def lemmatize_sentence(self, sentence):\n        lemmatized_words = []\n        sentence = self.remove_punctuation(sentence)\n        words = word_tokenize(sentence)\n        tagged_words = pos_tag(words)\n        for word, tag in tagged_words:\n            if tag.startswith('V'):\n                lemmatized_word = self.lemmatizer.lemmatize(word, pos='v')\n            elif tag.startswith('J'):\n                lemmatized_word = self.lemmatizer.lemmatize(word, pos='a')\n            elif tag.startswith('R'):\n                lemmatized_word = self.lemmatizer.lemmatize(word, pos='r')\n            else:\n                lemmatized_word = self.lemmatizer.lemmatize(word)\n            lemmatized_words.append(lemmatized_word)\n        return lemmatized_words",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [],
          "field_dependencies": [
            "self.lemmatizer"
          ],
          "method_dependencies": [
            "remove_punctuation"
          ]
        }
      },
      {
        "method_name": "get_pos_tag",
        "method_description": "def get_pos_tag(self, sentence):\n        \"\"\"\n        Remove punctuations of the sentence and tokenizes the input sentence, mark the part of speech tag of each word.\n        :param sentence: a sentence str\n        :return: list, part of speech tag of each word in the sentence.\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.get_pos_tag(\"I am running in a race.\")\n        ['PRP', 'VBP', 'VBG', 'IN', 'DT', 'NN']\n\n        \"\"\"",
        "test_class": "LemmatizationTestGetPosTag",
        "test_code": "class LemmatizationTestGetPosTag(unittest.TestCase):\n    def test_get_pos_tag_1(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"I am running in a race.\")\n        expected = ['PRP', 'VBP', 'VBG', 'IN', 'DT', 'NN']\n        self.assertEqual(result, expected)\n\n    def test_get_pos_tag_2(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"Cantanco's eyesight had been weak, but adequate.\")\n        expected = ['NNP', 'NN', 'VBD', 'VBN', 'JJ', 'CC', 'JJ']\n        self.assertEqual(result, expected)\n\n    def test_get_pos_tag_3(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"The dog's barked at the mailman.\")\n        expected = ['DT', 'NNS', 'VBD', 'IN', 'DT', 'NN']\n        self.assertEqual(result, expected)\n\n    def test_get_pos_tag_4(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"He was running and eating at same time. \")\n        expected = ['PRP', 'VBD', 'VBG', 'CC', 'VBG', 'IN', 'JJ', 'NN']\n        self.assertEqual(result, expected)\n\n    def test_get_pos_tag_5(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.get_pos_tag(\"I was taking a ride in the car.\")\n        expected = ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN']\n        self.assertEqual(result, expected)",
        "solution_code": "def get_pos_tag(self, sentence):\n        pos_tags = []\n        sentence = self.remove_punctuation(sentence)\n        words = word_tokenize(sentence)\n        tagged_words = pos_tag(words)\n        for tagged_word in tagged_words:\n            pos_tags.append(tagged_word[1])\n        return pos_tags",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [],
          "field_dependencies": [],
          "method_dependencies": [
            "remove_punctuation"
          ]
        }
      },
      {
        "method_name": "remove_punctuation",
        "method_description": "def remove_punctuation(self, sentence):\n        \"\"\"\n        Removes punctuation from the input text.\n        :param sentence: a sentence str\n        :return: str, sentence without any punctuation\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.remove_punctuation(\"I am running in a race.\")\n        'I am running in a race'\n\n        \"\"\"",
        "test_class": "LemmatizationTestRemovePunctuation",
        "test_code": "class LemmatizationTestRemovePunctuation(unittest.TestCase):\n    def test_remove_punctuation_1(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"I am running in a race.\")\n        expected = \"I am running in a race\"\n        self.assertEqual(result, expected)\n\n    def test_remove_punctuation_2(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"Until the beating, Cantanco's eyesight had been weak, but adequate.\")\n        expected = 'Until the beating Cantancos eyesight had been weak but adequate'\n        self.assertEqual(result, expected)\n\n    def test_remove_punctuation_3(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"The dog's barked at the mailman!!!\")\n        expected = 'The dogs barked at the mailman'\n        self.assertEqual(result, expected)\n\n    def test_remove_punctuation_4(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"He was running and eating at same time... \")\n        expected = 'He was running and eating at same time '\n        self.assertEqual(result, expected)\n\n    def test_remove_punctuation_5(self):\n        lemmatization = Lemmatization()\n        result = lemmatization.remove_punctuation(\"Is this a test? I hope it is...\")\n        expected = 'Is this a test I hope it is'\n        self.assertEqual(result, expected)",
        "solution_code": "def remove_punctuation(self, sentence):\n        return sentence.translate(str.maketrans('', '', string.punctuation))",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [
            "string"
          ],
          "field_dependencies": [],
          "method_dependencies": []
        }
      }
    ],
    "predict": [
      "```python\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag, word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\nnltk.download('averaged_perceptron_tagger', quiet=True)\nnltk.download('punkt', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True) # Often needed for wordnet\n\n\ndef get_wordnet_pos(tag):\n    \"\"\"Map Penn Treebank tag to WordNet tag.\"\"\"\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN # Default to noun if no mapping found\n\n\nclass Lemmatization:\n    \"\"\"\n    This is a class about Lemmatization, which utilizes the nltk library to perform lemmatization and part-of-speech tagging on sentences, as well as remove punctuation.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        creates a WordNetLemmatizer object and stores it in the self.lemmatizer member variable.\n        \"\"\"\n        self.lemmatizer = WordNetLemmatizer()\n\n    def remove_punctuation(self, sentence: str) -> str:\n        \"\"\"\n        Removes punctuation from the input text.\n        :param sentence: a sentence str\n        :return: str, sentence without any punctuation\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.remove_punctuation(\"I am running in a race.\")\n        'I am running in a race'\n        >>> lemmatization.remove_punctuation(\"Hello, world! This is a test.\")\n        'Hello world This is a test'\n        \"\"\"\n        # Create a translation table mapping punctuation to None\n        translator = str.maketrans('', '', string.punctuation)\n        # Apply the translation\n        return sentence.translate(translator)\n\n    def get_pos_tag(self, sentence: str) -> list[str]:\n        \"\"\"\n        Remove punctuations of the sentence and tokenizes the input sentence, mark the part of speech tag of each word.\n        :param sentence: a sentence str\n        :return: list, part of speech tag of each word in the sentence.\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.get_pos_tag(\"I am running in a race.\")\n        ['PRP', 'VBP', 'VBG', 'IN', 'DT', 'NN']\n        >>> lemmatization.get_pos_tag(\"The quick brown foxes jumped over the lazy dogs.\")\n        ['DT', 'JJ', 'JJ', 'NNS', 'VBD', 'IN', 'DT', 'JJ', 'NNS']\n        \"\"\"\n        # Remove punctuation\n        cleaned_sentence = self.remove_punctuation(sentence)\n        # Tokenize\n        tokens = word_tokenize(cleaned_sentence)\n        # Get POS tags\n        pos_tags_tuples = pos_tag(tokens)\n        # Extract only the tags\n        tags = [tag for word, tag in pos_tags_tuples]\n        return tags\n\n    def lemmatize_sentence(self, sentence: str) -> list[str]:\n        \"\"\"\n        Remove punctuations of the sentence and tokenizes the input sentence, mark the part of speech tag of each word,\n        lemmatizes the words with different parameters based on their parts of speech, and stores in a list.\n        :param sentence: a sentence str\n        :return: a list of words which have been lemmatized.\n        >>> lemmatization = Lemmatization()\n        >>> lemmatization.lemmatize_sentence(\"I am running in a race.\")\n        ['I', 'be', 'run', 'in', 'a', 'race']\n        >>> lemmatization.lemmatize_sentence(\"The quick brown foxes jumped over the lazy dogs.\")\n        ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']\n        \"\"\"\n        # Remove punctuation\n        cleaned_sentence = self.remove_punctuation(sentence)\n        # Tokenize\n        tokens = word_tokenize(cleaned_sentence)\n        # Get POS tags\n        pos_tags_tuples = pos_tag(tokens)\n\n        # Lemmatize based on POS\n        lemmatized_words = []\n        for word, tag in pos_tags_tuples:\n            # Map NLTK POS tag to WordNet tag\n            wordnet_tag = get_wordnet_pos(tag)\n            # Lemmatize the word using the appropriate tag\n            lemmatized_word = self.lemmatizer.lemmatize(word, pos=wordnet_tag)\n            lemmatized_words.append(lemmatized_word)\n\n        return lemmatized_words\n\n# Example Usage (optional, can be added outside the class definition for testing)\nif __name__ == '__main__':\n    lemmatizer_instance = Lemmatization()\n\n    sentence1 = \"I am running in a race.\"\n    print(f\"Original sentence: '{sentence1}'\")\n    print(f\"Removed punctuation: '{lemmatizer_instance.remove_punctuation(sentence1)}'\")\n    print(f\"POS tags: {lemmatizer_instance.get_pos_tag(sentence1)}\")\n    print(f\"Lemmatized sentence: {lemmatizer_instance.lemmatize_sentence(sentence1)}\")\n    print(\"-\" * 20)\n\n    sentence2 = \"The quick brown foxes jumped over the lazy dogs.\"\n    print(f\"Original sentence: '{sentence2}'\")\n    print(f\"Removed punctuation: '{lemmatizer_instance.remove_punctuation(sentence2)}'\")\n    print(f\"POS tags: {lemmatizer_instance.get_pos_tag(sentence2)}\")\n    print(f\"Lemmatized sentence: {lemmatizer_instance.lemmatize_sentence(sentence2)}\")\n    print(\"-\" * 20)\n\n    sentence3 = \"He is eating pizza.\"\n    print(f\"Original sentence: '{sentence3}'\")\n    print(f\"Removed punctuation: '{lemmatizer_instance.remove_punctuation(sentence3)}'\")\n    print(f\"POS tags: {lemmatizer_instance.get_pos_tag(sentence3)}\")\n    print(f\"Lemmatized sentence: {lemmatizer_instance.lemmatize_sentence(sentence3)}\")\n    print(\"-\" * 20)\n\n    # To run the doctests:\n    # import doctest\n    # doctest.testmod()\n```"
    ]
  },
  {
    "task_id": "ClassEval_53",
    "skeleton": "import re\nimport string\n\nclass LongestWord:\n    \"\"\"\n    This is a class allows to add words to a list and find the longest word in a given sentence by comparing the words with the ones in the word list.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize a list of word.\n        \"\"\"\n        self.word_list = []\n\n    def add_word(self, word):\n        \"\"\"\n        append the input word into self.word_list\n        :param word: str, input word\n        \"\"\"\n\n    def find_longest_word(self, sentence):\n        \"\"\"\n        Remove punctuation marks and split a sentence into a list of word. Find the longest splited word that is in the self.word_list.\n        Words are strictly case sensitive.\n        :param sentence: a sentence str\n        :return str: longest splited word that is in the self.word_list. return '' if self.word_list is empty.\n        >>> longestWord = LongestWord()\n        >>> longestWord.add_word('A')\n        >>> longestWord.add_word('aM')\n        >>> longestWord.find_longest_word('I am a student.')\n        'a'\n        \"\"\"",
    "test": "import unittest\n\nclass LongestWordTestAddWord(unittest.TestCase):\n    def test_add_word_1(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        self.assertEqual(['hello'], longestWord.word_list)\n\n    def test_add_word_2(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        longestWord.add_word(\"world\")\n        self.assertEqual(['hello', 'world'], longestWord.word_list)\n\n    def test_add_word_3(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        longestWord.add_word(\"world\")\n        longestWord.add_word(\"!\")\n        self.assertEqual(['hello', 'world', '!'], longestWord.word_list)\n\n    def test_add_word_4(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        longestWord.add_word(\"world\")\n        longestWord.add_word(\"!\")\n        longestWord.add_word(\"!\")\n        self.assertEqual(['hello', 'world', '!', '!'], longestWord.word_list)\n\n    def test_add_word_5(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        longestWord.add_word(\"world\")\n        longestWord.add_word(\"!\")\n        longestWord.add_word(\"!\")\n        longestWord.add_word(\"!\")\n        self.assertEqual(['hello', 'world', '!', '!', '!'], longestWord.word_list)\n\n\nclass LongestWordTestFindLongestWord(unittest.TestCase):\n    def test_find_longest_word_1(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"a\")\n        sentence = 'I am a student.'\n        self.assertEqual('a', longestWord.find_longest_word(sentence))\n\n    def test_find_longest_word_2(self):\n        longestWord = LongestWord()\n        sentence = 'I am a student.'\n        self.assertEqual('', longestWord.find_longest_word(sentence))\n\n    def test_find_longest_word_3(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"student\")\n        sentence = 'I am a student.'\n        self.assertEqual('student', longestWord.find_longest_word(sentence))\n\n    def test_find_longest_word_4(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"apple\")\n        sentence = 'Apple is red.'\n        self.assertEqual('apple', longestWord.find_longest_word(sentence))\n\n    def test_find_longest_word_5(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"apple\")\n        longestWord.add_word(\"red\")\n        sentence = 'Apple is red.'\n        self.assertEqual('apple', longestWord.find_longest_word(sentence))",
    "solution_code": "import re\nimport string\n\n\nclass LongestWord:\n\n    def __init__(self):\n        self.word_list = []\n\n    def add_word(self, word):\n        self.word_list.append(word)\n\n    def find_longest_word(self, sentence):\n        longest_word = \"\"\n        sentence = sentence.lower()\n        sentence = re.sub('[%s]' % re.escape(string.punctuation), '', sentence)\n        sentence = re.split(' ', sentence)\n        for word in sentence:\n            if word in self.word_list and len(word) > len(longest_word):\n                longest_word = word\n        return longest_word",
    "import_statement": [
      "import re",
      "import string"
    ],
    "class_description": "    \"\"\"\n    This is a class allows to add words to a list and find the longest word in a given sentence by comparing the words with the ones in the word list.\n    \"\"\"\n",
    "class_name": "LongestWord",
    "test_classes": [
      "LongestWordTestAddWord",
      "LongestWordTestFindLongestWord"
    ],
    "class_constructor": "class LongestWord: \n    def __init__(self):\n        \"\"\"\n        Initialize a list of word.\n        \"\"\"\n        self.word_list = []\n\n",
    "fields": [
      "self.word_list"
    ],
    "methods_info": [
      {
        "method_name": "add_word",
        "method_description": "def add_word(self, word):\n        \"\"\"\n        append the input word into self.word_list\n        :param word: str, input word\n        \"\"\"",
        "test_class": "LongestWordTestAddWord",
        "test_code": "class LongestWordTestAddWord(unittest.TestCase):\n    def test_add_word_1(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        self.assertEqual(['hello'], longestWord.word_list)\n\n    def test_add_word_2(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        longestWord.add_word(\"world\")\n        self.assertEqual(['hello', 'world'], longestWord.word_list)\n\n    def test_add_word_3(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        longestWord.add_word(\"world\")\n        longestWord.add_word(\"!\")\n        self.assertEqual(['hello', 'world', '!'], longestWord.word_list)\n\n    def test_add_word_4(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        longestWord.add_word(\"world\")\n        longestWord.add_word(\"!\")\n        longestWord.add_word(\"!\")\n        self.assertEqual(['hello', 'world', '!', '!'], longestWord.word_list)\n\n    def test_add_word_5(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"hello\")\n        longestWord.add_word(\"world\")\n        longestWord.add_word(\"!\")\n        longestWord.add_word(\"!\")\n        longestWord.add_word(\"!\")\n        self.assertEqual(['hello', 'world', '!', '!', '!'], longestWord.word_list)",
        "solution_code": "def add_word(self, word):\n        self.word_list.append(word)",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [],
          "field_dependencies": [
            "self.word_list"
          ],
          "method_dependencies": []
        }
      },
      {
        "method_name": "find_longest_word",
        "method_description": "def find_longest_word(self, sentence):\n        \"\"\"\n        Remove punctuation marks and split a sentence into a list of word. Find the longest splited word that is in the self.word_list.\n        Words are strictly case sensitive.\n        :param sentence: a sentence str\n        :return str: longest splited word that is in the self.word_list. return '' if self.word_list is empty.\n        >>> longestWord = LongestWord()\n        >>> longestWord.add_word('A')\n        >>> longestWord.add_word('aM')\n        >>> longestWord.find_longest_word('I am a student.')\n        'a'\n        \"\"\"",
        "test_class": "LongestWordTestFindLongestWord",
        "test_code": "class LongestWordTestFindLongestWord(unittest.TestCase):\n    def test_find_longest_word_1(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"a\")\n        sentence = 'I am a student.'\n        self.assertEqual('a', longestWord.find_longest_word(sentence))\n\n    def test_find_longest_word_2(self):\n        longestWord = LongestWord()\n        sentence = 'I am a student.'\n        self.assertEqual('', longestWord.find_longest_word(sentence))\n\n    def test_find_longest_word_3(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"student\")\n        sentence = 'I am a student.'\n        self.assertEqual('student', longestWord.find_longest_word(sentence))\n\n    def test_find_longest_word_4(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"apple\")\n        sentence = 'Apple is red.'\n        self.assertEqual('apple', longestWord.find_longest_word(sentence))\n\n    def test_find_longest_word_5(self):\n        longestWord = LongestWord()\n        longestWord.add_word(\"apple\")\n        longestWord.add_word(\"red\")\n        sentence = 'Apple is red.'\n        self.assertEqual('apple', longestWord.find_longest_word(sentence))",
        "solution_code": "def find_longest_word(self, sentence):\n        longest_word = \"\"\n        sentence = sentence.lower()\n        sentence = re.sub('[%s]' % re.escape(string.punctuation), '', sentence)\n        sentence = re.split(' ', sentence)\n        for word in sentence:\n            if word in self.word_list and len(word) > len(longest_word):\n                longest_word = word\n        return longest_word",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [
            "re",
            "string"
          ],
          "field_dependencies": [
            "self.word_list"
          ],
          "method_dependencies": []
        }
      }
    ],
    "predict": [
      "```python\nimport re\nimport string\n\nclass LongestWord:\n    \"\"\"\n    This is a class allows to add words to a list and find the longest word in a given sentence by comparing the words with the ones in the word list.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize a list of word.\n        \"\"\"\n        self.word_list = []\n\n    def add_word(self, word):\n        \"\"\"\n        append the input word into self.word_list\n        :param word: str, input word\n        \"\"\"\n        # Ensure the input is treated as a single word (e.g., no leading/trailing whitespace)\n        self.word_list.append(word.strip())\n\n    def find_longest_word(self, sentence):\n        \"\"\"\n        Remove punctuation marks and split a sentence into a list of word. Find the longest splited word that is in the self.word_list.\n        Words are strictly case sensitive.\n        :param sentence: a sentence str\n        :return str: longest splited word that is in the self.word_list. return '' if self.word_list is empty.\n        >>> longestWord = LongestWord()\n        >>> longestWord.add_word('A')\n        >>> longestWord.add_word('aM')\n        >>> longestWord.find_longest_word('I am a student.')\n        '' # Based on the description (case sensitive check) and the words added ('A', 'aM'),\n        # none of the words in the sentence ('I', 'am', 'a', 'student') are present in the word list.\n        # Therefore, the expected output based on the description is ''.\n        # The original doctest output 'a' contradicts the 'strictly case sensitive' rule and the added words.\n        # The code below implements the 'strictly case sensitive' rule as described.\n        >>> longestWord_case_sensitive = LongestWord()\n        >>> longestWord_case_sensitive.add_word('Apple')\n        >>> longestWord_case_sensitive.add_word('Banana')\n        >>> longestWord_case_sensitive.add_word('apple')\n        >>> longestWord_case_sensitive.add_word('Orange')\n        >>> longestWord_case_sensitive.find_longest_word('I like Apple, Banana and apple.')\n        'Banana' # 'Apple' and 'apple' are in the list, 'Banana' is in the list. 'Banana' is the longest match.\n        >>> longestWord_empty = LongestWord()\n        >>> longestWord_empty.find_longest_word('This is a sentence.')\n        '' # Empty word list returns ''\n        >>> longestWord_no_match = LongestWord()\n        >>> longestWord_no_match.add_word('test')\n        >>> longestWord_no_match.find_longest_word('No words match here.')\n        '' # No word from sentence found in word list\n        \"\"\"\n        # Handle empty word list\n        if not self.word_list:\n            return ''\n\n        # For efficient lookups, convert the word list to a set.\n        # This uses O(N) space but allows O(1) average time lookups.\n        # We create it here to ensure it's up-to-date if add_word was called.\n        word_list_set = set(self.word_list)\n\n\n        # Remove punctuation marks from the sentence\n        # Using string.punctuation and translate is generally efficient\n        translator = str.maketrans('', '', string.punctuation)\n        cleaned_sentence = sentence.translate(translator)\n\n        # Split the cleaned sentence into words\n        # Use split() without arguments to handle multiple spaces and trim whitespace\n        sentence_words = cleaned_sentence.split()\n\n        longest_found_word = ''\n        max_length = 0\n\n        # Find the longest word from sentence_words that is strictly case sensitive in self.word_list\n        for word in sentence_words:\n            # Check if the word from the sentence is exactly present in the word list set\n            if word in word_list_set:\n                # If it is present, compare its length with the current longest found word\n                if len(word) > max_length:\n                    max_length = len(word)\n                    longest_found_word = word\n                # Optional: If tie-breaking is needed (not specified, but common),\n                # one might add `or (len(word) == max_length and word < longest_found_word)`\n                # for alphabetical order, or similar. Sticking to finding *any* longest.\n\n        return longest_found_word\n\n# Example usage (beyond the doctests):\nif __name__ == '__main__':\n    lw = LongestWord()\n    lw.add_word(\"hello\")\n    lw.add_word(\"world\")\n    lw.add_word(\"a\")\n    lw.add_word(\"test\")\n    lw.add_word(\"program\")\n\n    sentence1 = \"hello, this is a test program.\"\n    print(f\"Sentence: '{sentence1}'\")\n    print(f\"Word List: {lw.word_list}\")\n    print(f\"Longest found word: '{lw.find_longest_word(sentence1)}'\") # Expected: program\n\n    sentence2 = \"hi there world!\"\n    print(f\"\\nSentence: '{sentence2}'\")\n    print(f\"Word List: {lw.word_list}\")\n    print(f\"Longest found word: '{lw.find_longest_word(sentence2)}'\") # Expected: world\n\n    sentence3 = \"no matching words here.\"\n    print(f\"\\nSentence: '{sentence3}'\")\n    print(f\"Word List: {lw.word_list}\")\n    print(f\"Longest found word: '{lw.find_longest_word(sentence3)}'\") # Expected: ''\n\n    lw_case = LongestWord()\n    lw_case.add_word(\"Word\")\n    lw_case.add_word(\"another\")\n    lw_case.add_word(\"LongerWord\")\n    sentence4 = \"A Word and another one, also a LongerWord.\"\n    print(f\"\\nSentence: '{sentence4}'\")\n    print(f\"Word List: {lw_case.word_list}\")\n    print(f\"Longest found word: '{lw_case.find_longest_word(sentence4)}'\") # Expected: LongerWord\n\n    sentence5 = \"word vs ANOTHER\"\n    print(f\"\\nSentence: '{sentence5}'\")\n    print(f\"Word List: {lw_case.word_list}\")\n    print(f\"Longest found word: '{lw_case.find_longest_word(sentence5)}'\") # Expected: '' (due to case sensitivity)\n```"
    ]
  },
  {
    "task_id": "ClassEval_62",
    "skeleton": "class NLPDataProcessor:\n    \"\"\"\n    The class processes NLP data by removing stop words from a list of strings using a pre-defined stop word list.\n    \"\"\"\n\n\n    def construct_stop_word_list(self):\n        \"\"\"\n        Construct a stop word list including 'a', 'an', 'the'.\n        :return: a list of stop words\n        >>> NLPDataProcessor.construct_stop_word_list()\n        ['a', 'an', 'the']\n        \"\"\"\n    def remove_stop_words(self, string_list, stop_word_list):\n        \"\"\"\n        Remove all the stop words from the list of strings.\n        :param string_list: a list of strings\n        :param stop_word_list: a list of stop words\n        :return: a list of words without stop words\n        >>> NLPDataProcessor.process(['This is a test.'])\n        [['This', 'is', 'test.']]\n        \"\"\"\n    def process(self, string_list):\n        \"\"\"\n        Construct a stop word list including 'a', 'an', 'the', and remove all the stop words from the list of strings.\n        :param string_list: a list of strings\n        :return: a list of words without stop words\n        >>> NLPDataProcessor.process(['This is a test.'])\n        [['This', 'is', 'test.']]\n        \"\"\"",
    "test": "import unittest\n\nclass NLPDataProcessorTestConstruct(unittest.TestCase):\n    def setUp(self):\n        self.processor = NLPDataProcessor()\n\n    def test_construct_stop_word_list(self):\n        stop_word_list = self.processor.construct_stop_word_list()\n        expected_stop_words = ['a', 'an', 'the']\n        self.assertEqual(stop_word_list, expected_stop_words)\n\nclass NLPDataProcessorTestRemove(unittest.TestCase):\n    def setUp(self):\n        self.processor = NLPDataProcessor()\n\n    def test_remove_stop_words(self):\n        string_list = ['This is a test', 'This is an apple', 'This is the dog']\n        stop_word_list = ['a', 'an', 'the']\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        expected_words_list = [['This', 'is', 'test'], ['This', 'is', 'apple'], ['This', 'is', 'dog']]\n        self.assertEqual(words_list, expected_words_list)\n\n    def test_remove_stop_words_2(self):\n        string_list = ['a', 'an', 'the']\n        stop_word_list = ['a', 'an', 'the']\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        self.assertEqual(words_list, [[], [], []])\n\n    def test_remove_stop_words_3(self):\n        string_list = []\n        stop_word_list = ['a', 'an', 'the']\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        self.assertEqual(words_list, [])\n\n    def test_remove_stop_words_4(self):\n        string_list = ['This is a test', 'This is an apple', 'This is the dog']\n        stop_word_list = []\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        expected_words_list = [['This', 'is', 'a', 'test'], ['This', 'is', 'an', 'apple'], ['This', 'is', 'the', 'dog']]\n        self.assertEqual(words_list, expected_words_list)\n\n    def test_remove_stop_words_5(self):\n        string_list = ['This is a test', 'This is an apple', 'This is the dog']\n        stop_word_list = ['a', 'an', 'the', 'This', 'is']\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        expected_words_list = [['is', 'test'], ['is', 'apple'], ['is', 'dog']]\n        self.assertEqual(words_list, expected_words_list)\n\nclass NLPDataProcessorTestProcess(unittest.TestCase):\n    def setUp(self):\n        self.processor = NLPDataProcessor()\n\n    def test_process(self):\n        string_list = ['This is a test.', 'This is an apple.', 'This is the dog.']\n        words_list = self.processor.process(string_list)\n        expected_words_list = [['This', 'is', 'test.'], ['This', 'is', 'apple.'], ['This', 'is', 'dog.']]\n        self.assertEqual(words_list, expected_words_list)\n\n    def test_process_with_empty_string_list(self):\n        string_list = []\n        words_list = self.processor.process(string_list)\n        self.assertEqual(words_list, [])\n\n    def test_process_with_single_word_sentences(self):\n        string_list = ['Hello aa', 'World']\n        words_list = self.processor.process(string_list)\n        expected_words_list = [['Hello', 'aa'], ['World']]\n        self.assertEqual(words_list, expected_words_list)\n\n    def test_process_with_stop_words_only(self):\n        string_list = ['a', 'an', 'the']\n        words_list = self.processor.process(string_list)\n        self.assertEqual(words_list, [[], [], []])\n\n    def test_process_with_stop_words_only_2(self):\n        string_list = ['a', 'an', 'the','This']\n        words_list = self.processor.process(string_list)\n        self.assertEqual(words_list,[[], [], [], ['This']])",
    "solution_code": "class NLPDataProcessor:\n\n    def construct_stop_word_list(self):\n        stop_word_list = ['a', 'an', 'the']\n        return stop_word_list\n\n    def remove_stop_words(self, string_list, stop_word_list):\n        answer = []\n        for string in string_list:\n            string_split = string.split()\n            for word in string_split:\n                if word in stop_word_list:\n                    string_split.remove(word)\n            answer.append(string_split)\n        return answer\n\n    def process(self, string_list):\n        stop_word_list = self.construct_stop_word_list()\n        words_list = self.remove_stop_words(string_list, stop_word_list)\n        return words_list",
    "import_statement": [],
    "class_description": "    \"\"\"\n    The class processes NLP data by removing stop words from a list of strings using a pre-defined stop word list.\n    \"\"\"\n",
    "class_name": "NLPDataProcessor",
    "test_classes": [
      "NLPDataProcessorTestConstruct",
      "NLPDataProcessorTestRemove",
      "NLPDataProcessorTestProcess"
    ],
    "class_constructor": "class NLPDataProcessor: \n",
    "fields": [],
    "methods_info": [
      {
        "method_name": "construct_stop_word_list",
        "method_description": "def construct_stop_word_list(self):\n        \"\"\"\n        Construct a stop word list including 'a', 'an', 'the'.\n        :return: a list of stop words\n        >>> NLPDataProcessor.construct_stop_word_list()\n        ['a', 'an', 'the']\n        \"\"\"",
        "test_class": "NLPDataProcessorTestConstruct",
        "test_code": "class NLPDataProcessorTestConstruct(unittest.TestCase):\n    def setUp(self):\n        self.processor = NLPDataProcessor()\n\n    def test_construct_stop_word_list(self):\n        stop_word_list = self.processor.construct_stop_word_list()\n        expected_stop_words = ['a', 'an', 'the']\n        self.assertEqual(stop_word_list, expected_stop_words)",
        "solution_code": "def construct_stop_word_list(self):\n        stop_word_list = ['a', 'an', 'the']\n        return stop_word_list",
        "dependencies": {
          "Standalone": true,
          "lib_dependencies": [],
          "field_dependencies": [],
          "method_dependencies": []
        }
      },
      {
        "method_name": "remove_stop_words",
        "method_description": "def remove_stop_words(self, string_list, stop_word_list):\n        \"\"\"\n        Remove all the stop words from the list of strings.\n        :param string_list: a list of strings\n        :param stop_word_list: a list of stop words\n        :return: a list of words without stop words\n        >>> NLPDataProcessor.process(['This is a test.'])\n        [['This', 'is', 'test.']]\n        \"\"\"",
        "test_class": "NLPDataProcessorTestRemove",
        "test_code": "class NLPDataProcessorTestRemove(unittest.TestCase):\n    def setUp(self):\n        self.processor = NLPDataProcessor()\n\n    def test_remove_stop_words(self):\n        string_list = ['This is a test', 'This is an apple', 'This is the dog']\n        stop_word_list = ['a', 'an', 'the']\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        expected_words_list = [['This', 'is', 'test'], ['This', 'is', 'apple'], ['This', 'is', 'dog']]\n        self.assertEqual(words_list, expected_words_list)\n\n    def test_remove_stop_words_2(self):\n        string_list = ['a', 'an', 'the']\n        stop_word_list = ['a', 'an', 'the']\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        self.assertEqual(words_list, [[], [], []])\n\n    def test_remove_stop_words_3(self):\n        string_list = []\n        stop_word_list = ['a', 'an', 'the']\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        self.assertEqual(words_list, [])\n\n    def test_remove_stop_words_4(self):\n        string_list = ['This is a test', 'This is an apple', 'This is the dog']\n        stop_word_list = []\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        expected_words_list = [['This', 'is', 'a', 'test'], ['This', 'is', 'an', 'apple'], ['This', 'is', 'the', 'dog']]\n        self.assertEqual(words_list, expected_words_list)\n\n    def test_remove_stop_words_5(self):\n        string_list = ['This is a test', 'This is an apple', 'This is the dog']\n        stop_word_list = ['a', 'an', 'the', 'This', 'is']\n        words_list = self.processor.remove_stop_words(string_list, stop_word_list)\n        expected_words_list = [['is', 'test'], ['is', 'apple'], ['is', 'dog']]\n        self.assertEqual(words_list, expected_words_list)",
        "solution_code": "def remove_stop_words(self, string_list, stop_word_list):\n        answer = []\n        for string in string_list:\n            string_split = string.split()\n            for word in string_split:\n                if word in stop_word_list:\n                    string_split.remove(word)\n            answer.append(string_split)\n        return answer",
        "dependencies": {
          "Standalone": true,
          "lib_dependencies": [],
          "field_dependencies": [],
          "method_dependencies": []
        }
      },
      {
        "method_name": "process",
        "method_description": "def process(self, string_list):\n        \"\"\"\n        Construct a stop word list including 'a', 'an', 'the', and remove all the stop words from the list of strings.\n        :param string_list: a list of strings\n        :return: a list of words without stop words\n        >>> NLPDataProcessor.process(['This is a test.'])\n        [['This', 'is', 'test.']]\n        \"\"\"",
        "test_class": "NLPDataProcessorTestProcess",
        "test_code": "class NLPDataProcessorTestProcess(unittest.TestCase):\n    def setUp(self):\n        self.processor = NLPDataProcessor()\n\n    def test_process(self):\n        string_list = ['This is a test.', 'This is an apple.', 'This is the dog.']\n        words_list = self.processor.process(string_list)\n        expected_words_list = [['This', 'is', 'test.'], ['This', 'is', 'apple.'], ['This', 'is', 'dog.']]\n        self.assertEqual(words_list, expected_words_list)\n\n    def test_process_with_empty_string_list(self):\n        string_list = []\n        words_list = self.processor.process(string_list)\n        self.assertEqual(words_list, [])\n\n    def test_process_with_single_word_sentences(self):\n        string_list = ['Hello aa', 'World']\n        words_list = self.processor.process(string_list)\n        expected_words_list = [['Hello', 'aa'], ['World']]\n        self.assertEqual(words_list, expected_words_list)\n\n    def test_process_with_stop_words_only(self):\n        string_list = ['a', 'an', 'the']\n        words_list = self.processor.process(string_list)\n        self.assertEqual(words_list, [[], [], []])\n\n    def test_process_with_stop_words_only_2(self):\n        string_list = ['a', 'an', 'the','This']\n        words_list = self.processor.process(string_list)\n        self.assertEqual(words_list,[[], [], [], ['This']])",
        "solution_code": "def process(self, string_list):\n        stop_word_list = self.construct_stop_word_list()\n        words_list = self.remove_stop_words(string_list, stop_word_list)\n        return words_list",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [],
          "field_dependencies": [],
          "method_dependencies": [
            "construct_stop_word_list",
            "remove_stop_words"
          ]
        }
      }
    ],
    "predict": [
      "```python\nimport re # Import regex for potentially better word splitting if needed, but simple split seems sufficient for doctest\n\nclass NLPDataProcessor:\n    \"\"\"\n    The class processes NLP data by removing stop words from a list of strings using a pre-defined stop word list.\n    \"\"\"\n\n    @staticmethod\n    def construct_stop_word_list():\n        \"\"\"\n        Construct a stop word list including 'a', 'an', 'the'.\n        :return: a list of stop words\n        >>> NLPDataProcessor.construct_stop_word_list()\n        ['a', 'an', 'the']\n        \"\"\"\n        return ['a', 'an', 'the']\n\n    @staticmethod\n    def remove_stop_words(string_list, stop_word_list):\n        \"\"\"\n        Remove all the stop words from the list of strings.\n        Splits each string into words (simple whitespace split).\n        Stop word matching is case-insensitive.\n        :param string_list: a list of strings\n        :param stop_word_list: a list of stop words\n        :return: a list of lists of words without stop words, corresponding to the input strings\n        \"\"\"\n        # Convert stop word list to a set for efficient lookup and make it lowercase\n        stop_words = set(word.lower() for word in stop_word_list)\n\n        processed_strings = []\n        for text in string_list:\n            # Simple split by whitespace. More complex tokenization might be needed for real NLP.\n            words = text.split()\n            \n            # Filter out stop words (case-insensitive check)\n            filtered_words = [word for word in words if word.lower() not in stop_words]\n            \n            processed_strings.append(filtered_words)\n\n        return processed_strings\n\n    @staticmethod\n    def process(string_list):\n        \"\"\"\n        Construct a stop word list including 'a', 'an', 'the', and remove all the stop words from the list of strings.\n        :param string_list: a list of strings\n        :return: a list of lists of words without stop words, corresponding to the input strings\n        >>> NLPDataProcessor.process(['This is a test.'])\n        [['This', 'is', 'test.']]\n        >>> NLPDataProcessor.process(['An apple and a pear.'])\n        [['Apple', 'and', 'pear.']]\n        >>> NLPDataProcessor.process(['The quick brown fox.'])\n        [['quick', 'brown', 'fox.']]\n        >>> NLPDataProcessor.process(['List with an item and a another item and the end.'])\n        [['List', 'with', 'item', 'another', 'item', 'end.']]\n        >>> NLPDataProcessor.process([''])\n        [[]]\n        >>> NLPDataProcessor.process([])\n        []\n        \"\"\"\n        # Construct the default stop word list\n        stop_word_list = NLPDataProcessor.construct_stop_word_list()\n\n        # Remove stop words using the constructed list\n        processed_data = NLPDataProcessor.remove_stop_words(string_list, stop_word_list)\n\n        return processed_data\n\n# Example usage (optional, for demonstration)\nif __name__ == '__main__':\n    processor = NLPDataProcessor() # Although methods are static, you can still instantiate\n\n    # Using construct_stop_word_list (static method)\n    default_stops = NLPDataProcessor.construct_stop_word_list()\n    print(f\"Default stop words: {default_stops}\")\n\n    # Using process (static method)\n    sample_data = [\n        \"This is a sample sentence.\",\n        \"Another sentence with an article.\",\n        \"The final test string.\"\n    ]\n    processed_output = NLPDataProcessor.process(sample_data)\n    print(f\"\\nOriginal data: {sample_data}\")\n    print(f\"Processed data: {processed_output}\")\n\n    # Running doctests\n    import doctest\n    doctest.testmod(verbose=True)\n```"
    ]
  },
  {
    "task_id": "ClassEval_63",
    "skeleton": "import re\nfrom collections import Counter\n\nclass NLPDataProcessor2:\n    \"\"\"\n    The class processes NLP data by extracting words from a list of strings, calculating the frequency of each word, and returning the top 5 most frequent words.\n    \"\"\"\n\n\n    def process_data(self, string_list):\n        \"\"\"\n        keep only English letters and spaces in the string, then convert the string to lower case, and then split the string into a list of words.\n        :param string_list: a list of strings\n        :return: words_list: a list of words lists\n        >>> NLPDataProcessor.process_data(['This is a test.'])\n        [['this', 'is', 'a', 'test']]\n        \"\"\"\n\n    def calculate_word_frequency(self, words_list):\n        \"\"\"\n        Calculate the word frequency of each word in the list of words list, and sort the word frequency dictionary by value in descending order.\n        :param words_list: a list of words lists\n        :return: top 5 word frequency dictionary, a dictionary of word frequency, key is word, value is frequency\n        >>> NLPDataProcessor.calculate_word_frequency([['this', 'is', 'a', 'test'], ['this', 'is', 'another', 'test']])\n        {'this': 2, 'is': 2, 'test': 2, 'a': 1, 'another': 1}\n        \"\"\"\n\n    def process(self, string_list):\n        \"\"\"\n        keep only English letters and spaces in the string, then convert the string to lower case, and then split the string into a list of words. Calculate the word frequency of each word in the list of words list, and sort the word frequency dictionary by value in descending order.\n        :param string_list: a list of strings\n        :return: top 5 word frequency dictionary, a dictionary of word frequency, key is word, value is frequency\n        >>> NLPDataProcessor.process(['This is a test.', 'This is another test.'])\n        {'this': 2, 'is': 2, 'test': 2, 'a': 1, 'another': 1}\n        \"\"\"",
    "test": "import unittest\n\nclass NLPDataProcessorTestProcessData(unittest.TestCase):\n\n    def setUp(self):\n        self.processor = NLPDataProcessor2()\n\n    def test_process_data(self):\n        string_list = [\"Hello World!\", \"This is a test.\"]\n        expected_output = [['hello', 'world'], ['this', 'is', 'a', 'test']]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process_data2(self):\n        string_list = [\"12345\", \"Special@Characters\"]\n        expected_output = [[], ['specialcharacters']]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process_data3(self):\n        string_list = []\n        expected_output = []\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process_data4(self):\n        string_list = [\"Hello World!\", \"This is a test.\", \"12345\", \"Special@Characters\"]\n        expected_output = [['hello', 'world'], ['this', 'is', 'a', 'test'], [], ['specialcharacters']]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process_data5(self):\n        string_list = [\"Hello World!\", \"This is a test.\", \"12345\", \"Special@Characters\", \"Hello World!\", \"This is a test.\", \"12345\", \"Special@Characters\"]\n        expected_output = [['hello', 'world'], ['this', 'is', 'a', 'test'], [], ['specialcharacters'], ['hello', 'world'], ['this', 'is', 'a', 'test'], [], ['specialcharacters']]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\nclass NLPDataProcessorTestCalculate(unittest.TestCase):\n\n    def setUp(self):\n        self.processor = NLPDataProcessor2()\n\n    def test_calculate_word_frequency(self):\n        words_list = [['hello', 'world'], ['this', 'is', 'a', 'test'], ['hello', 'world', 'this', 'is', 'another', 'test'],\n                      ['hello', 'hello', 'world']]\n        expected_output = {'hello': 4, 'world': 3, 'this': 2, 'is': 2, 'test': 2}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_calculate_word_frequency2(self):\n        words_list = [['hello', 'world'], ['this', 'is', 'a', 'test'], ['hello', 'world', 'this', 'is', 'another', 'test'],\n                      ['hello', 'hello', 'world'], ['world', 'world', 'world']]\n        expected_output = {'world': 6, 'hello': 4, 'this': 2, 'is': 2, 'test': 2}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_calculate_word_frequency3(self):\n        words_list = [['hello', 'world'], ['hello', 'hello', 'world'], ['world', 'world']]\n        expected_output = {'world': 4, 'hello': 3}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_calculate_word_frequency4(self):\n        words_list = [['hello', 'world'], ['this', 'is', 'a', '%%%'], ['hello', 'world', 'this', 'is', 'another', '%%%'],\n                      ['hello', 'hello', 'world'], ['%%%', 'world', 'a', '%%%'], ['%%%', 'hello', '%%%']]\n        expected_output = {'%%%': 6, 'hello': 5, 'world': 4, 'is': 2, 'this': 2}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_calculate_word_frequency5(self):\n        words_list = [['hello', 'world'], ['this', 'is', 'a', '%%%'], ['hello', 'world', 'this', 'is', 'another', '%%%'],\n                      ['hello', 'hello', 'world'], ['%%%', 'world', 'a', '%%%'], ['%%%', 'hello', '%%%'], ['hello', 'world'], ['this', 'is', 'a', '%%%'], ['hello', 'world', 'this', 'is', 'another', '%%%'],\n                      ['hello', 'hello', 'world'], ['%%%', 'world', 'a', '%%%'], ['%%%', 'hello', '%%%']]\n        expected_output = {'%%%': 12, 'hello': 10, 'world': 8, 'is': 4, 'this': 4}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\nclass NLPDataProcessorTestProcess(unittest.TestCase):\n\n    def setUp(self):\n        self.processor = NLPDataProcessor2()\n\n    def test_process(self):\n        string_list = [\"Hello World!\", \"This is a test.\", \"Hello World, this is a test.\"]\n        expected_output = {'hello': 2, 'world': 2, 'this': 2, 'is': 2, 'a': 2}\n        self.assertEqual(self.processor.process(string_list), expected_output)\n\n    def test_process2(self):\n        string_list = []\n        expected_output = []\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_calculate3(self):\n        words_list = []\n        expected_output = {}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_process4(self):\n        string_list = [\"@#$%^&*\", \"Special_Characters\", \"12345\"]\n        expected_output = [[], ['specialcharacters'], []]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process5(self):\n        string_list = [\"Hello World! %%%\", \"This is a %%% test. %%% \", \"Hello World, this is a test. %%%\"]\n        expected_output = {'hello': 2, 'world': 2, 'this': 2, 'is': 2, 'a': 2}\n        self.assertEqual(self.processor.process(string_list), expected_output)\n\n    def test_process6(self):\n        string_list = [\"12345\", \"67890\", \"98765\"]\n        expected_output = [[], [], []]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)",
    "solution_code": "from collections import Counter\nimport re\n\nclass NLPDataProcessor2:\n\n    def process_data(self, string_list):\n        words_list = []\n        for string in string_list:\n            # Remove non-English letters and convert to lowercase\n            processed_string = re.sub(r'[^a-zA-Z\\s]', '', string.lower())\n            # Split the string into words\n            words = processed_string.split()\n            words_list.append(words)\n        return words_list\n\n    def calculate_word_frequency(self, words_list):\n        word_frequency = Counter()\n        for words in words_list:\n            word_frequency.update(words)\n        sorted_word_frequency = dict(sorted(word_frequency.items(), key=lambda x: x[1], reverse=True))\n        top_5_word_frequency = dict(list(sorted_word_frequency.items())[:5])\n        return top_5_word_frequency\n\n    def process(self, string_list):\n        words_list = self.process_data(string_list)\n        word_frequency_dict = self.calculate_word_frequency(words_list)\n        return word_frequency_dict",
    "import_statement": [
      "from collections import Counter",
      "import re"
    ],
    "class_description": "    \"\"\"\n    The class processes NLP data by extracting words from a list of strings, calculating the frequency of each word, and returning the top 5 most frequent words.\n    \"\"\"\n",
    "class_name": "NLPDataProcessor2",
    "test_classes": [
      "NLPDataProcessorTestProcessData",
      "NLPDataProcessorTestCalculate",
      "NLPDataProcessorTestProcess"
    ],
    "class_constructor": "class NLPDataProcessor2: \n",
    "fields": [],
    "methods_info": [
      {
        "method_name": "process_data",
        "method_description": "def process_data(self, string_list):\n        \"\"\"\n        keep only English letters and spaces in the string, then convert the string to lower case, and then split the string into a list of words.\n        :param string_list: a list of strings\n        :return: words_list: a list of words lists\n        >>> NLPDataProcessor.process_data(['This is a test.'])\n        [['this', 'is', 'a', 'test']]\n        \"\"\"",
        "test_class": "NLPDataProcessorTestProcessData",
        "test_code": "class NLPDataProcessorTestProcessData(unittest.TestCase):\n\n    def setUp(self):\n        self.processor = NLPDataProcessor2()\n\n    def test_process_data(self):\n        string_list = [\"Hello World!\", \"This is a test.\"]\n        expected_output = [['hello', 'world'], ['this', 'is', 'a', 'test']]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process_data2(self):\n        string_list = [\"12345\", \"Special@Characters\"]\n        expected_output = [[], ['specialcharacters']]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process_data3(self):\n        string_list = []\n        expected_output = []\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process_data4(self):\n        string_list = [\"Hello World!\", \"This is a test.\", \"12345\", \"Special@Characters\"]\n        expected_output = [['hello', 'world'], ['this', 'is', 'a', 'test'], [], ['specialcharacters']]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process_data5(self):\n        string_list = [\"Hello World!\", \"This is a test.\", \"12345\", \"Special@Characters\", \"Hello World!\", \"This is a test.\", \"12345\", \"Special@Characters\"]\n        expected_output = [['hello', 'world'], ['this', 'is', 'a', 'test'], [], ['specialcharacters'], ['hello', 'world'], ['this', 'is', 'a', 'test'], [], ['specialcharacters']]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)",
        "solution_code": "def process_data(self, string_list):\n        words_list = []\n        for string in string_list:\n            # Remove non-English letters and convert to lowercase\n            processed_string = re.sub(r'[^a-zA-Z\\s]', '', string.lower())\n            # Split the string into words\n            words = processed_string.split()\n            words_list.append(words)\n        return words_list",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [
            "re"
          ],
          "field_dependencies": [],
          "method_dependencies": [
            "process"
          ]
        }
      },
      {
        "method_name": "calculate_word_frequency",
        "method_description": "def calculate_word_frequency(self, words_list):\n        \"\"\"\n        Calculate the word frequency of each word in the list of words list, and sort the word frequency dictionary by value in descending order.\n        :param words_list: a list of words lists\n        :return: top 5 word frequency dictionary, a dictionary of word frequency, key is word, value is frequency\n        >>> NLPDataProcessor.calculate_word_frequency([['this', 'is', 'a', 'test'], ['this', 'is', 'another', 'test']])\n        {'this': 2, 'is': 2, 'test': 2, 'a': 1, 'another': 1}\n        \"\"\"",
        "test_class": "NLPDataProcessorTestCalculate",
        "test_code": "class NLPDataProcessorTestCalculate(unittest.TestCase):\n\n    def setUp(self):\n        self.processor = NLPDataProcessor2()\n\n    def test_calculate_word_frequency(self):\n        words_list = [['hello', 'world'], ['this', 'is', 'a', 'test'], ['hello', 'world', 'this', 'is', 'another', 'test'],\n                      ['hello', 'hello', 'world']]\n        expected_output = {'hello': 4, 'world': 3, 'this': 2, 'is': 2, 'test': 2}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_calculate_word_frequency2(self):\n        words_list = [['hello', 'world'], ['this', 'is', 'a', 'test'], ['hello', 'world', 'this', 'is', 'another', 'test'],\n                      ['hello', 'hello', 'world'], ['world', 'world', 'world']]\n        expected_output = {'world': 6, 'hello': 4, 'this': 2, 'is': 2, 'test': 2}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_calculate_word_frequency3(self):\n        words_list = [['hello', 'world'], ['hello', 'hello', 'world'], ['world', 'world']]\n        expected_output = {'world': 4, 'hello': 3}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_calculate_word_frequency4(self):\n        words_list = [['hello', 'world'], ['this', 'is', 'a', '%%%'], ['hello', 'world', 'this', 'is', 'another', '%%%'],\n                      ['hello', 'hello', 'world'], ['%%%', 'world', 'a', '%%%'], ['%%%', 'hello', '%%%']]\n        expected_output = {'%%%': 6, 'hello': 5, 'world': 4, 'is': 2, 'this': 2}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_calculate_word_frequency5(self):\n        words_list = [['hello', 'world'], ['this', 'is', 'a', '%%%'], ['hello', 'world', 'this', 'is', 'another', '%%%'],\n                      ['hello', 'hello', 'world'], ['%%%', 'world', 'a', '%%%'], ['%%%', 'hello', '%%%'], ['hello', 'world'], ['this', 'is', 'a', '%%%'], ['hello', 'world', 'this', 'is', 'another', '%%%'],\n                      ['hello', 'hello', 'world'], ['%%%', 'world', 'a', '%%%'], ['%%%', 'hello', '%%%']]\n        expected_output = {'%%%': 12, 'hello': 10, 'world': 8, 'is': 4, 'this': 4}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)",
        "solution_code": "def calculate_word_frequency(self, words_list):\n        word_frequency = Counter()\n        for words in words_list:\n            word_frequency.update(words)\n        sorted_word_frequency = dict(sorted(word_frequency.items(), key=lambda x: x[1], reverse=True))\n        top_5_word_frequency = dict(list(sorted_word_frequency.items())[:5])\n        return top_5_word_frequency",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [
            "Counter"
          ],
          "field_dependencies": [],
          "method_dependencies": []
        }
      },
      {
        "method_name": "process",
        "method_description": "def process(self, string_list):\n        \"\"\"\n        keep only English letters and spaces in the string, then convert the string to lower case, and then split the string into a list of words. Calculate the word frequency of each word in the list of words list, and sort the word frequency dictionary by value in descending order.\n        :param string_list: a list of strings\n        :return: top 5 word frequency dictionary, a dictionary of word frequency, key is word, value is frequency\n        >>> NLPDataProcessor.process(['This is a test.', 'This is another test.'])\n        {'this': 2, 'is': 2, 'test': 2, 'a': 1, 'another': 1}\n        \"\"\"",
        "test_class": "NLPDataProcessorTestProcess",
        "test_code": "class NLPDataProcessorTestProcess(unittest.TestCase):\n\n    def setUp(self):\n        self.processor = NLPDataProcessor2()\n\n    def test_process(self):\n        string_list = [\"Hello World!\", \"This is a test.\", \"Hello World, this is a test.\"]\n        expected_output = {'hello': 2, 'world': 2, 'this': 2, 'is': 2, 'a': 2}\n        self.assertEqual(self.processor.process(string_list), expected_output)\n\n    def test_process2(self):\n        string_list = []\n        expected_output = []\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_calculate3(self):\n        words_list = []\n        expected_output = {}\n        self.assertEqual(self.processor.calculate_word_frequency(words_list), expected_output)\n\n    def test_process4(self):\n        string_list = [\"@#$%^&*\", \"Special_Characters\", \"12345\"]\n        expected_output = [[], ['specialcharacters'], []]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)\n\n    def test_process5(self):\n        string_list = [\"Hello World! %%%\", \"This is a %%% test. %%% \", \"Hello World, this is a test. %%%\"]\n        expected_output = {'hello': 2, 'world': 2, 'this': 2, 'is': 2, 'a': 2}\n        self.assertEqual(self.processor.process(string_list), expected_output)\n\n    def test_process6(self):\n        string_list = [\"12345\", \"67890\", \"98765\"]\n        expected_output = [[], [], []]\n        self.assertEqual(self.processor.process_data(string_list), expected_output)",
        "solution_code": "def process(self, string_list):\n        words_list = self.process_data(string_list)\n        word_frequency_dict = self.calculate_word_frequency(words_list)\n        return word_frequency_dict",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [],
          "field_dependencies": [],
          "method_dependencies": [
            "process_data",
            "calculate_word_frequency"
          ]
        }
      }
    ],
    "predict": [
      "```python\nimport re\nfrom collections import Counter\n\nclass NLPDataProcessor2:\n    \"\"\"\n    The class processes NLP data by extracting words from a list of strings, calculating the frequency of each word, and returning the top 5 most frequent words.\n    \"\"\"\n\n\n    def process_data(self, string_list):\n        \"\"\"\n        keep only English letters and spaces in the string, then convert the string to lower case, and then split the string into a list of words.\n        :param string_list: a list of strings\n        :return: words_list: a list of lists of words\n        >>> processor = NLPDataProcessor2()\n        >>> processor.process_data(['This is a test.', 'Another test!'])\n        [['this', 'is', 'a', 'test'], ['another', 'test']]\n        >>> processor.process_data(['Hello World!', ' Python  is fun.'])\n        [['hello', 'world'], ['python', 'is', 'fun']]\n        >>> processor.process_data(['', ' Just spaces '])\n        [[], ['just', 'spaces']]\n        \"\"\"\n        words_list = []\n        for s in string_list:\n            # Keep only English letters and spaces\n            cleaned_s = re.sub('[^a-zA-Z ]', '', s)\n            # Convert to lower case\n            lowered_s = cleaned_s.lower()\n            # Split into words, handling multiple spaces\n            words = lowered_s.split()\n            words_list.append(words)\n        return words_list\n\n    def calculate_word_frequency(self, words_list):\n        \"\"\"\n        Calculate the word frequency of each word across all word lists,\n        and sort the word frequency dictionary by value in descending order.\n        Note: The example output shows the full dictionary, not just top 5.\n        :param words_list: a list of lists of words\n        :return: A dictionary of word frequency, key is word, value is frequency, sorted by frequency descending.\n        >>> processor = NLPDataProcessor2()\n        >>> processor.calculate_word_frequency([['this', 'is', 'a', 'test'], ['this', 'is', 'another', 'test']])\n        {'this': 2, 'is': 2, 'test': 2, 'a': 1, 'another': 1}\n        >>> processor.calculate_word_frequency([['hello', 'world'], ['python', 'is', 'fun'], ['hello', 'python']])\n        {'hello': 2, 'python': 2, 'world': 1, 'is': 1, 'fun': 1}\n        >>> processor.calculate_word_frequency([[]])\n        {}\n        >>> processor.calculate_word_frequency([])\n        {}\n        \"\"\"\n        all_words = [word for sublist in words_list for word in sublist]\n        \n        # Count word frequencies\n        word_counts = Counter(all_words)\n        \n        # Get items sorted by frequency descending\n        # most_common() returns a list of (word, count) tuples sorted by count\n        sorted_items = word_counts.most_common()\n        \n        # Convert the sorted list of tuples back to a dictionary\n        # In Python 3.7+, dictionary order is guaranteed to be insertion order\n        sorted_freq_dict = dict(sorted_items)\n        \n        return sorted_freq_dict\n\n    def process(self, string_list):\n        \"\"\"\n        keep only English letters and spaces in the string, then convert the string to lower case,\n        and then split the string into a list of words. Calculate the word frequency of each word\n        in the list of words list, and sort the word frequency dictionary by value in descending order.\n        :param string_list: a list of strings\n        :return: A dictionary of word frequency, key is word, value is frequency, sorted by frequency descending.\n        >>> processor = NLPDataProcessor2()\n        >>> processor.process(['This is a test.', 'This is another test.'])\n        {'this': 2, 'is': 2, 'test': 2, 'a': 1, 'another': 1}\n        >>> processor.process(['Hello World!', ' Python  is fun.', 'Hello Python.'])\n        {'hello': 2, 'python': 2, 'world': 1, 'is': 1, 'fun': 1}\n        >>> processor.process(['', ' !@#$ '])\n        {}\n        \"\"\"\n        # Step 1: Process data into list of word lists\n        words_list = self.process_data(string_list)\n        \n        # Step 2: Calculate word frequency\n        freq_dict = self.calculate_word_frequency(words_list)\n        \n        return freq_dict\n\n# Example usage:\nif __name__ == '__main__':\n    processor = NLPDataProcessor2()\n\n    data = [\n        \"This is the first sentence.\",\n        \"This is the second sentence, and it is different.\",\n        \"The third sentence is here.\"\n    ]\n\n    # Test process_data\n    processed_words = processor.process_data(data)\n    print(\"Processed Data:\")\n    print(processed_words)\n    print(\"-\" * 20)\n\n    # Test calculate_word_frequency\n    word_freq = processor.calculate_word_frequency(processed_words)\n    print(\"Word Frequency (sorted):\")\n    print(word_freq)\n    print(\"-\" * 20)\n\n    # Test process (end-to-end)\n    full_result = processor.process(data)\n    print(\"Full Process Result:\")\n    print(full_result)\n\n    # Run doctests\n    import doctest\n    doctest.testmod()\n```"
    ]
  },
  {
    "task_id": "ClassEval_97",
    "skeleton": "class Words2Numbers:\n    \"\"\"\n    The class provides a text-to-number conversion utility, allowing conversion of written numbers (in words) to their numerical representation.\n    \"\"\"\n\n\n    def __init__(self):\n        \"\"\"\n        Initialize the word lists and dictionaries required for conversion\n        \"\"\"\n        self.numwords = {}\n        self.units = [\n            \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n            \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n            \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n        ]\n        self.tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n        self.scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n\n        self.numwords[\"and\"] = (1, 0)\n        for idx, word in enumerate(self.units):\n            self.numwords[word] = (1, idx)\n        for idx, word in enumerate(self.tens):\n            self.numwords[word] = (1, idx * 10)\n        for idx, word in enumerate(self.scales):\n            self.numwords[word] = (10 ** (idx * 3 or 2), 0)\n\n        self.ordinal_words = {'first': 1, 'second': 2, 'third': 3, 'fifth': 5, 'eighth': 8, 'ninth': 9, 'twelfth': 12}\n        self.ordinal_endings = [('ieth', 'y'), ('th', '')]\n\n\n    def text2int(self, textnum):\n        \"\"\"\n        Convert the word string to the corresponding integer string\n        :param textnum: string, the word string to be converted\n        :return: string, the final converted integer string\n        >>> w2n = Words2Numbers()\n        >>> w2n.text2int(\"thirty-two\")\n        \"32\"\n        \"\"\"\n\n    def is_valid_input(self, textnum):\n        \"\"\"\n        Check if the input text contains only valid words that can be converted into numbers.\n        :param textnum: The input text containing words representing numbers.\n        :return: True if input is valid, False otherwise.\n        >>> w2n = Words2Numbers()\n        >>> w2n.is_valid_input(\"thirty-two\")\n        False\n        \"\"\"",
    "test": "import unittest\n\n\nclass Words2NumbersTestText2Int(unittest.TestCase):\n    def test_text2int(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"thirty-two\"), \"32\")\n\n    def test_text2int2(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"one hundred and twenty-three\"), \"123\")\n\n    def test_text2int3(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"two thousand and nineteen\"), \"2019\")\n\n    def test_text2int4(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"one hundred and one\"), \"101\")\n\n    def test_text2int5(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"one million and eleven\"), \"1000011\")\n\n    def test_text2int6(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"one million one hundred sixty-ninth\"), \"1000169\")\n\nclass Words2NumbersTestIsValidInput(unittest.TestCase):\n    def test_is_valid_input(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"twenty-five thousand three hundred and forty-two\"))\n\n    def test_is_valid_input2(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"second hundred and third\"))\n\n    def test_is_valid_input3(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"twenty-fifth thousand three hundred and forty-second\"))\n\n    def test_is_valid_input4(self):\n        w2n = Words2Numbers()\n        self.assertFalse(w2n.is_valid_input(\"eleventy thousand and five\"))\n\n    def test_is_valid_input5(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"seventy two thousand and hundred eleven\"))\n\n    def test_is_valid_input6(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"fifteenth hundred\"))\n\nclass  Words2NumbersTestMain(unittest.TestCase):\n    def test_main(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.is_valid_input(\"seventy two thousand and hundred eleven\"), True)\n        self.assertEqual(w2n.text2int(\"seventy two thousand and hundred eleven\"), \"72011\")",
    "solution_code": "class Words2Numbers:\n\n    def __init__(self):\n        self.numwords = {}\n        self.units = [\n            \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n            \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n            \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n        ]\n        self.tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n        self.scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n\n        self.numwords[\"and\"] = (1, 0)\n        for idx, word in enumerate(self.units):\n            self.numwords[word] = (1, idx)\n        for idx, word in enumerate(self.tens):\n            self.numwords[word] = (1, idx * 10)\n        for idx, word in enumerate(self.scales):\n            self.numwords[word] = (10 ** (idx * 3 or 2), 0)\n\n        self.ordinal_words = {'first': 1, 'second': 2, 'third': 3, 'fifth': 5, 'eighth': 8, 'ninth': 9, 'twelfth': 12}\n        self.ordinal_endings = [('ieth', 'y'), ('th', '')]\n\n    def text2int(self, textnum):\n        textnum = textnum.replace('-', ' ')\n\n        current = result = 0\n        curstring = \"\"\n        onnumber = False\n        for word in textnum.split():\n            if word in self.ordinal_words:\n                scale, increment = (1, self.ordinal_words[word])\n                current = current * scale + increment\n                onnumber = True\n            else:\n                for ending, replacement in self.ordinal_endings:\n                    if word.endswith(ending):\n                        word = \"%s%s\" % (word[:-len(ending)], replacement)\n\n                if word not in self.numwords:\n                    if onnumber:\n                        curstring += repr(result + current) + \" \"\n                    curstring += word + \" \"\n                    result = current = 0\n                    onnumber = False\n                else:\n                    scale, increment = self.numwords[word]\n                    current = current * scale + increment\n                    if scale > 100:\n                        result += current\n                        current = 0\n                    onnumber = True\n\n        if onnumber:\n            curstring += repr(result + current)\n\n        return curstring\n\n    def is_valid_input(self, textnum):\n\n        textnum = textnum.replace('-', ' ')\n\n        for word in textnum.split():\n            if word in self.ordinal_words:\n                continue\n            else:\n                for ending, replacement in self.ordinal_endings:\n                    if word.endswith(ending):\n                        word = \"%s%s\" % (word[:-len(ending)], replacement)\n\n                if word not in self.numwords:\n                    return False\n\n        return True",
    "import_statement": [],
    "class_description": "    \"\"\"\n    The class provides a text-to-number conversion utility, allowing conversion of written numbers (in words) to their numerical representation.\n    \"\"\"\n",
    "class_name": "Words2Numbers",
    "test_classes": [
      "Words2NumbersTestText2Int",
      "Words2NumbersTestIsValidInput",
      " Words2NumbersTestMain"
    ],
    "class_constructor": "class Words2Numbers: \n    def __init__(self):\n        \"\"\"\n        Initialize the word lists and dictionaries required for conversion\n        \"\"\"\n        self.numwords = {}\n        self.units = [\n            \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n            \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n            \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n        ]\n        self.tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n        self.scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n\n        self.numwords[\"and\"] = (1, 0)\n        for idx, word in enumerate(self.units):\n            self.numwords[word] = (1, idx)\n        for idx, word in enumerate(self.tens):\n            self.numwords[word] = (1, idx * 10)\n        for idx, word in enumerate(self.scales):\n            self.numwords[word] = (10 ** (idx * 3 or 2), 0)\n\n        self.ordinal_words = {'first': 1, 'second': 2, 'third': 3, 'fifth': 5, 'eighth': 8, 'ninth': 9, 'twelfth': 12}\n        self.ordinal_endings = [('ieth', 'y'), ('th', '')]\n\n\n",
    "fields": [
      "self.numwords",
      "self.ordinal_endings",
      "self.ordinal_words",
      "self.scales",
      "self.tens",
      "self.units"
    ],
    "methods_info": [
      {
        "method_name": "text2int",
        "method_description": "def text2int(self, textnum):\n        \"\"\"\n        Convert the word string to the corresponding integer string\n        :param textnum: string, the word string to be converted\n        :return: string, the final converted integer string\n        >>> w2n = Words2Numbers()\n        >>> w2n.text2int(\"thirty-two\")\n        \"32\"\n        \"\"\"",
        "test_class": "Words2NumbersTestText2Int",
        "test_code": "class Words2NumbersTestText2Int(unittest.TestCase):\n    def test_text2int(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"thirty-two\"), \"32\")\n\n    def test_text2int2(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"one hundred and twenty-three\"), \"123\")\n\n    def test_text2int3(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"two thousand and nineteen\"), \"2019\")\n\n    def test_text2int4(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"one hundred and one\"), \"101\")\n\n    def test_text2int5(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"one million and eleven\"), \"1000011\")\n\n    def test_text2int6(self):\n        w2n = Words2Numbers()\n        self.assertEqual(w2n.text2int(\"one million one hundred sixty-ninth\"), \"1000169\")",
        "solution_code": "def text2int(self, textnum):\n        textnum = textnum.replace('-', ' ')\n\n        current = result = 0\n        curstring = \"\"\n        onnumber = False\n        for word in textnum.split():\n            if word in self.ordinal_words:\n                scale, increment = (1, self.ordinal_words[word])\n                current = current * scale + increment\n                onnumber = True\n            else:\n                for ending, replacement in self.ordinal_endings:\n                    if word.endswith(ending):\n                        word = \"%s%s\" % (word[:-len(ending)], replacement)\n\n                if word not in self.numwords:\n                    if onnumber:\n                        curstring += repr(result + current) + \" \"\n                    curstring += word + \" \"\n                    result = current = 0\n                    onnumber = False\n                else:\n                    scale, increment = self.numwords[word]\n                    current = current * scale + increment\n                    if scale > 100:\n                        result += current\n                        current = 0\n                    onnumber = True\n\n        if onnumber:\n            curstring += repr(result + current)\n\n        return curstring",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [],
          "field_dependencies": [
            "self.numwords",
            "self.ordinal_endings",
            "self.ordinal_words"
          ],
          "method_dependencies": []
        }
      },
      {
        "method_name": "is_valid_input",
        "method_description": "def is_valid_input(self, textnum):\n        \"\"\"\n        Check if the input text contains only valid words that can be converted into numbers.\n        :param textnum: The input text containing words representing numbers.\n        :return: True if input is valid, False otherwise.\n        >>> w2n = Words2Numbers()\n        >>> w2n.is_valid_input(\"thirty-two\")\n        False\n        \"\"\"",
        "test_class": "Words2NumbersTestIsValidInput",
        "test_code": "class Words2NumbersTestIsValidInput(unittest.TestCase):\n    def test_is_valid_input(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"twenty-five thousand three hundred and forty-two\"))\n\n    def test_is_valid_input2(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"second hundred and third\"))\n\n    def test_is_valid_input3(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"twenty-fifth thousand three hundred and forty-second\"))\n\n    def test_is_valid_input4(self):\n        w2n = Words2Numbers()\n        self.assertFalse(w2n.is_valid_input(\"eleventy thousand and five\"))\n\n    def test_is_valid_input5(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"seventy two thousand and hundred eleven\"))\n\n    def test_is_valid_input6(self):\n        w2n = Words2Numbers()\n        self.assertTrue(w2n.is_valid_input(\"fifteenth hundred\"))",
        "solution_code": "def is_valid_input(self, textnum):\n\n        textnum = textnum.replace('-', ' ')\n\n        for word in textnum.split():\n            if word in self.ordinal_words:\n                continue\n            else:\n                for ending, replacement in self.ordinal_endings:\n                    if word.endswith(ending):\n                        word = \"%s%s\" % (word[:-len(ending)], replacement)\n\n                if word not in self.numwords:\n                    return False\n\n        return True",
        "dependencies": {
          "Standalone": false,
          "lib_dependencies": [],
          "field_dependencies": [
            "self.numwords",
            "self.ordinal_endings",
            "self.ordinal_words"
          ],
          "method_dependencies": []
        }
      }
    ],
    "predict": [
      "```python\nimport re\n\nclass Words2Numbers:\n    \"\"\"\n    The class provides a text-to-number conversion utility, allowing conversion of written numbers (in words) to their numerical representation.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the word lists and dictionaries required for conversion\n        \"\"\"\n        self.numwords = {}\n        self.units = [\n            \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n            \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n            \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n        ]\n        self.tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n        # Scales: hundred is 10^2, others are 10^(3*n)\n        self.scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n        self.major_scales = set(self.scales[1:]) # Set of major scales for quick lookup\n\n        # Populate numwords dictionary\n        self.numwords[\"and\"] = (1, 0) # Multiplier 1, add 0 (effectively ignores \"and\" in value calculation)\n        for idx, word in enumerate(self.units):\n            self.numwords[word] = (1, idx) # Multiplier 1, add value (0-19)\n        for idx, word in enumerate(self.tens):\n            self.numwords[word] = (1, idx * 10) # Multiplier 1, add value (20, 30, ...)\n        # Scale values: hundred=100, thousand=1000, million=1000000, ...\n        # The logic calculates 10**(0*3 or 2)=100, 10**(1*3 or 2)=1000, 10**(2*3 or 2)=10**6, etc.\n        for idx, word in enumerate(self.scales):\n            self.numwords[word] = (10 ** (idx * 3 or 2), 0) # Multiplier is scale value, add 0\n\n\n        # These are defined but not used in the current text2int implementation,\n        # which focuses on cardinal numbers based on the provided example.\n        self.ordinal_words = {'first': 1, 'second': 2, 'third': 3, 'fifth': 5, 'eighth': 8, 'ninth': 9, 'twelfth': 12}\n        self.ordinal_endings = [('ieth', 'y'), ('th', '')] # Not used in current implementation\n\n        # Combine all valid word keys for validation in is_valid_input\n        self._valid_word_set = set(self.numwords.keys())\n        self._valid_word_set.update(self.ordinal_words.keys())\n\n\n    def text2int(self, textnum):\n        \"\"\"\n        Convert the word string to the corresponding integer string.\n\n        Supports cardinal numbers up to trillions, including hyphenated forms\n        and the word \"and\". Handles standard English number phrasing.\n\n        :param textnum: string, the word string to be converted.\n        :return: string, the final converted integer string.\n        :raises ValueError: If the input is not a string, is empty, or contains\n                            unrecognizable words or invalid number sequences.\n\n        >>> w2n = Words2Numbers()\n        >>> w2n.text2int(\"thirty-two\")\n        '32'\n        >>> w2n.text2int(\"one hundred twenty three\")\n        '123'\n        >>> w2n.text2int(\"two thousand five hundred and forty-two\")\n        '2542'\n        >>> w2n.text2int(\"one million two hundred fifty thousand three hundred\")\n        '1250300'\n        >>> w2n.text2int(\"zero\")\n        '0'\n        >>> w2n.text2int(\"one billion\")\n        '1000000000'\n        >>> w2n.text2int(\"one hundred and twenty\")\n        '120'\n        >>> w2n.text2int(\"one thousand and five\")\n        '1005'\n        >>> w2n.text2int(\"thousand\") # Handles standalone major scale\n        '1000'\n        >>> w2n.text2int(\"million\") # Handles standalone major scale\n        '1000000'\n        >>> w2n.text2int(\"hundred\") # Handles standalone hundred (as 100)\n        '100'\n        >>> w2n.text2int(\"twenty one\")\n        '21'\n        >>> w2n.text2int(\"forty\")\n        '40'\n        >>> w2n.text2int(\"five hundred\")\n        '500'\n        \"\"\"\n        if not isinstance(textnum, str) or not textnum.strip():\n            # Handle empty or non-string input\n            raise ValueError(\"Input must be a non-empty string.\")\n\n        # Preprocess input: lowercase and replace hyphens with spaces\n        # Using regex split handles multiple spaces nicely\n        cleaned_text = textnum.lower().replace('-', ' ')\n        words = re.split(r'\\s+', cleaned_text.strip())\n\n        total = 0              # Accumulates value from major scales (thousands, millions...)\n        current_segment_base = 0 # Accumulates value from hundreds (e.g., 100, 200) within the current segment (0-999 part)\n        current_hundreds_part = 0 # Accumulates value from units/tens (e.g., 23, 5, 40) within the current segment (0-999 part)\n\n\n        for word in words:\n            # The re.split and strip combination should prevent empty words, but added check for robustness\n            if not word:\n                 continue\n\n            if word not in self.numwords:\n                # Allow conversion of simple ordinals if they map directly to a unit word\n                # e.g., 'first' -> 'one', 'third' -> 'three'. 'hundredth', 'thousandth' not handled.\n                if word in self.ordinal_words:\n                     try:\n                         # Attempt to find the cardinal equivalent word in units\n                         cardinal_value = self.ordinal_words[word]\n                         if 0 <= cardinal_value < len(self.units):\n                              word = self.units[cardinal_value]\n                         else:\n                              # If ordinal value doesn't map to a simple unit, raise error\n                              raise ValueError(f\"Invalid or unhandled ordinal word: {word}\")\n                     except Exception:\n                           # Catch potential errors during mapping\n                           raise ValueError(f\"Invalid or unhandled ordinal word: {word}\")\n                else:\n                    raise ValueError(f\"Invalid number word: {word}\")\n\n\n            scale, value = self.numwords[word]\n\n            if word in self.major_scales: # thousand, million, billion, trillion\n                # Before processing the major scale, finalize the current segment value (hundreds_part + segment_base).\n                segment_value = current_segment_base + current_hundreds_part\n\n                # If segment_value is 0 here, it implies a major scale like \"thousand\" or \"million\" appeared\n                # without a preceding number (e.g., \"thousand\", \"thousand hundred\").\n                # \"thousand\" alone should be 1000. \"million\" alone should be 1,000,000 etc. Treat as \"one\" of that scale.\n                segment_to_add = segment_value if segment_value > 0 else 1\n\n                total += segment_to_add * scale\n                current_segment_base = 0 # Reset for the next segment below this major scale\n                current_hundreds_part = 0 # Reset for the next segment below this major scale\n\n            elif word == \"hundred\":\n                 # 'hundred' multiplies the value accumulated so far in the current hundreds_part (which should be 1-99).\n                 # The result becomes the new base for the current segment (current_segment_base).\n                 # Example: \"one hundred\" -> chp=1. 'hundred' -> csb becomes 1*100=100, chp resets.\n                 # Example: \"twenty five hundred\" -> chp=25. 'hundred' -> csb becomes 25*100=2500, chp resets.\n                 # If current_hundreds_part is 0 before 'hundred', it means a sequence like \"hundred twenty\".\n                 # Standard English requires a number before \"hundred\" (e.g., \"one hundred\").\n                 # However, a standalone \"hundred\" means 100. Let's allow \"hundred\" alone or after a major scale reset (when chp is 0), treating it as 100.\n                 if current_hundreds_part == 0:\n                     # If chp is 0, 'hundred' implies 100. Add 100 to the base.\n                     current_segment_base += scale # scale for 'hundred' is 100\n                 else:\n                     # If chp > 0, 'hundred' multiplies the value in chp.\n                     current_segment_base += current_hundreds_part * scale\n                     current_hundreds_part = 0 # Clear the hundreds part accumulator after applying hundred\n\n            elif word == \"and\":\n                # \"and\" is typically ignored in value calculation.\n                # The (1, 0) mapping results in no change to current_hundreds_part.\n                pass # No action needed\n\n            else: # Units (0-19) or Tens (20-90)\n                # These add to the current_hundreds_part accumulator (which represents the 0-99 value).\n                # Sequence checks:\n                # 1. Cannot add a unit (1-9) if current_hundreds_part is not 0 AND not a multiple of 10.\n                #    (e.g., \"thirteen five\" (13 + 5 -> 18, invalid); \"twenty three five\" (23 + 5 -> 28, invalid)).\n                #    This check `current_hundreds_part % 10 != 0` fails for values like 13, 23, 123 etc.\n                if 1 <= value <= 9 and current_hundreds_part > 0 and current_hundreds_part % 10 != 0:\n                     raise ValueError(f\"Invalid number word sequence: '{word}' cannot follow '{self._find_last_word_value(current_hundreds_part)}'.\")\n                     # Helper `_find_last_word_value` is not strictly needed, just helps error message. Simpler:\n                     # raise ValueError(f\"Invalid number word sequence: unit '{word}' cannot follow {current_hundreds_part}.\")\n\n                # 2. Cannot add a teen (10-19) if current_hundreds_part is not 0.\n                #    (e.g., \"twenty fifteen\" (20 + 15 -> 35, invalid)).\n                if 10 <= value <= 19 and current_hundreds_part != 0:\n                     raise ValueError(f\"Invalid number word sequence: teen '{word}' cannot follow {current_hundreds_part}.\")\n\n                # 3. Cannot add a ten (20-90) if current_hundreds_part is not 0.\n                #    (e.g., \"five twenty\" (5 + 20 -> 25, invalid); \"twenty thirty\" (20 + 30 -> 50, invalid)).\n                if value > 0 and value % 10 == 0 and current_hundreds_part != 0:\n                    raise ValueError(f\"Invalid number word sequence: ten '{word}' cannot follow {current_hundreds_part}.\")\n\n                # If checks pass, add the value to current_hundreds_part.\n                current_hundreds_part += value\n\n        # After the loop, add the final segment value (hundreds_part + segment_base) to total\n        total += current_segment_base + current_hundreds_part\n\n        return str(total)\n\n    # Helper method (optional, for more informative error messages if needed)\n    # def _find_last_word_value(self, val):\n    #      # Find the word corresponding to the last added value <= val\n    #      # This is complex as multiple words add up (e.g., 23 from 'twenty' and 'three')\n    #      # Let's skip this for simplicity. The value in the error message is usually enough.\n    #      return val\n\n\n    def is_valid_input(self, textnum):\n        \"\"\"\n        Check if the input text contains only valid words that can be converted into numbers.\n        This method checks if each space-separated word (after converting to lowercase)\n        is present in the known set of number words (cardinals, 'and', and some ordinals).\n        Hyphenated words (like \"thirty-two\") are treated as single words and\n        are considered invalid by this check unless explicitly in the word lists\n        (which they are not by default setup).\n        Note: This does *not* check if the *sequence* of words forms a grammatically correct\n        or parseable number phrase (e.g., \"hundred hundred\" passes this check).\n        The text2int method provides sequence validation.\n\n        :param textnum: The input text containing words representing numbers.\n        :return: True if all space-separated words are valid number words, False otherwise.\n\n        >>> w2n = Words2Numbers()\n        >>> w2n.is_valid_input(\"thirty-two\")\n        False\n        >>> w2n.is_valid_input(\"thirty two\")\n        True\n        >>> w2n.is_valid_input(\"one hundred\")\n        True\n        >>> w2n.is_valid_input(\"two thousand five hundred and forty two\") # Note: forty two without hyphen\n        True\n        >>> w2n.is_valid_input(\"thirty apple\")\n        False\n        >>> w2n.is_valid_input(\"first\")\n        True\n        >>> w2n.is_valid_input(\"hundred hundred\") # Sequence is invalid, but words are valid\n        True\n        >>> w2n.is_valid_input(\"\")\n        True\n        >>> w2n.is_valid_input(\" \")\n        True\n        \"\"\"\n        if not isinstance(textnum, str):\n             return False\n\n        # Convert to lowercase and split by spaces\n        words = textnum.lower().split()\n\n        # Check if all non-empty words are in the set of valid number words\n        for word in words:\n            word = word.strip() # Clean up potential whitespace\n            if word and word not in self._valid_word_set:\n                return False\n\n        # If loop completes, all words were either empty or in the valid set\n        return True\n\n```"
    ]
  }
]